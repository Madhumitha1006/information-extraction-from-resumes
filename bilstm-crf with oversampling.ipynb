{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de5e756c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from gensim.models import word2vec, FastText\n",
    "import gensim.downloader\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "484fa761",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('resume_entities.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3e23cef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "O                        54234\n",
       "I-Skills                  2571\n",
       "I-College Name             572\n",
       "I-Degree                   407\n",
       "I-Designation              384\n",
       "B-Skills                   297\n",
       "B-College Name             290\n",
       "B-Degree                   279\n",
       "B-Designation              244\n",
       "I-Name                     226\n",
       "B-Name                     215\n",
       "B-Companies worked at      212\n",
       "B-Location                 199\n",
       "B-Email Address            189\n",
       "B-Graduation Year          156\n",
       "I-Companies worked at      139\n",
       "I-Email Address             75\n",
       "B-Years of Experience       32\n",
       "I-Years of Experience       25\n",
       "I-Location                   4\n",
       "B-UNKNOWN                    1\n",
       "Name: tag, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['tag'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d38bd658",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming 'df' is your DataFrame with 'token' and 'tag' columns\n",
    "\n",
    "# Calculate the current value counts of 'O' tags\n",
    "current_o_count = df['tag'].value_counts()['O']\n",
    "\n",
    "if current_o_count > 3000:\n",
    "    # Number of 'O' tags to remove to make their count equal to 3000\n",
    "    tags_to_remove = current_o_count - 3000\n",
    "\n",
    "    # Get the indices of 'O' tags\n",
    "    o_indices = df[df['tag'] == 'O'].index\n",
    "\n",
    "    # Convert the Int64Index to a list and then sample and remove excess 'O' tags\n",
    "    indices_to_remove = o_indices.tolist()[:tags_to_remove]\n",
    "    df_balanced = df.drop(indices_to_remove)\n",
    "else:\n",
    "    # No need to remove any 'O' tags, as their count is already less than or equal to 3000\n",
    "    df_balanced = df.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80687314",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "O                        3000\n",
       "I-Skills                 2571\n",
       "I-College Name            572\n",
       "I-Degree                  407\n",
       "I-Designation             384\n",
       "B-Skills                  297\n",
       "B-College Name            290\n",
       "B-Degree                  279\n",
       "B-Designation             244\n",
       "I-Name                    226\n",
       "B-Name                    215\n",
       "B-Companies worked at     212\n",
       "B-Location                199\n",
       "B-Email Address           189\n",
       "B-Graduation Year         156\n",
       "I-Companies worked at     139\n",
       "I-Email Address            75\n",
       "B-Years of Experience      32\n",
       "I-Years of Experience      25\n",
       "I-Location                  4\n",
       "B-UNKNOWN                   1\n",
       "Name: tag, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=df_balanced\n",
    "df['tag'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0082d3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'df' is your DataFrame with 'token' and 'tag' columns\n",
    "\n",
    "# Calculate the maximum count among all tags\n",
    "max_size = df['tag'].value_counts().max()\n",
    "\n",
    "# Initialize an empty list to store the over-sampled DataFrames\n",
    "lst = []\n",
    "\n",
    "# Group the DataFrame by 'tag'\n",
    "grouped = df.groupby('tag')\n",
    "\n",
    "# Perform over-sampling for each group (tag)\n",
    "for tag, group in grouped:\n",
    "    if len(group) < max_size:\n",
    "        lst.append(group.sample(max_size - len(group), replace=True))\n",
    "        \n",
    "# Concatenate the original DataFrame with the over-sampled DataFrames\n",
    "df = pd.concat([df] + lst)\n",
    "\n",
    "# Shuffle the rows of the balanced DataFrame to randomize the order\n",
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Now, 'balanced_df' contains the DataFrame with balanced class distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d09f7ca4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "I-Email Address          3000\n",
       "B-Degree                 3000\n",
       "B-Years of Experience    3000\n",
       "B-UNKNOWN                3000\n",
       "I-Years of Experience    3000\n",
       "B-Companies worked at    3000\n",
       "B-Location               3000\n",
       "I-Degree                 3000\n",
       "B-Name                   3000\n",
       "B-Designation            3000\n",
       "B-Email Address          3000\n",
       "I-Skills                 3000\n",
       "O                        3000\n",
       "B-College Name           3000\n",
       "I-Name                   3000\n",
       "I-Location               3000\n",
       "B-Skills                 3000\n",
       "B-Graduation Year        3000\n",
       "I-College Name           3000\n",
       "I-Companies worked at    3000\n",
       "I-Designation            3000\n",
       "Name: tag, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['tag'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd62502f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences are 218\n",
      "Number of Vocabs are 5084\n"
     ]
    }
   ],
   "source": [
    "print('Number of sentences are '+str(df['sentence'].nunique()))\n",
    "print('Number of Vocabs are '+str(df['token'].nunique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "83405811",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "import gensim.downloader as api\n",
    "from tensorflow.keras.layers import Embedding\n",
    "\n",
    "tf.random.set_seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "97bc60ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['token']=df['token'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "26761ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_all = [(list(zip(group['token'], group['tag']))) for _, group in df.groupby('sentence')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "64096669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum value of sentence counts: 3324\n",
      "Minimum value of sentence counts: 70\n"
     ]
    }
   ],
   "source": [
    "# Calculate the value counts for the \"sentence\" column\n",
    "sentence_counts = df['sentence'].value_counts()\n",
    "\n",
    "# Find the maximum value of the value counts\n",
    "max_count = sentence_counts.max()\n",
    "min_count = sentence_counts.min()\n",
    "\n",
    "# Print the maximum value\n",
    "print(\"Maximum value of sentence counts:\", max_count)\n",
    "print(\"Minimum value of sentence counts:\", min_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8d79c8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 3330\n",
    "EMBEDDING = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "61a52ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = list(set(df['token'].values))\n",
    "tags = list(set(df[\"tag\"].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5b488a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating word to index and tag to index\n",
    "import sklearn\n",
    "\n",
    "# Vocabulary Key:word -> Value:token_index\n",
    "# The first 2 entries are reserved for PAD and UNK\n",
    "word2idx = {w: i + 2 for i, w in enumerate(words)}\n",
    "word2idx[\"UNK\"] = 1 # Unknown words\n",
    "word2idx[\"PAD\"] = 0 # Padding\n",
    "\n",
    "# Vocabulary Key:token_index -> Value:word\n",
    "idx2word = {i: w for w, i in word2idx.items()}\n",
    "\n",
    "# Vocabulary Key:Label/Tag -> Value:tag_index\n",
    "# The first entry is reserved for PAD\n",
    "tag2idx = {t: i+1 for i, t in enumerate(tags)}\n",
    "tag2idx[\"PAD\"] = 0\n",
    "\n",
    "# Vocabulary Key:tag_index -> Value:Label/Tag\n",
    "idx2tag = {i: w for w, i in tag2idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7b7ab419",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "651b94f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = {}  # Define the 'vocab' dictionary before the loop\n",
    "with open('glove.840B.300d.txt', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        if len(values) < 2:\n",
    "            continue\n",
    "        word = values[0]\n",
    "        try:\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            word2vec[word] = coefs\n",
    "        except ValueError:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f2755e4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 2619 words (1822 misses)\n"
     ]
    }
   ],
   "source": [
    "# Embedding the word matrix, we will add this\n",
    "hits = 0\n",
    "misses = 0\n",
    "# word2vec = api.load(\"word2vec-google-news-300\")\n",
    "embedding_dim = 300\n",
    "\n",
    "# Prepare embedding matrix\n",
    "embedding_matrix = np.zeros((len(word2idx), embedding_dim))\n",
    "for word, i in zip(word2idx.keys(),word2idx.values()):\n",
    "    embedding_vector = None\n",
    "    try:\n",
    "      embedding_vector = word2vec[word]\n",
    "    except Exception :\n",
    "      pass\n",
    "\n",
    "    if embedding_vector is not None:\n",
    "        # Words not found in embedding index will be all-zeros.\n",
    "        # This includes the representation for \"padding\" and \"OOV\"\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        hits += 1\n",
    "    else:\n",
    "        misses += 1\n",
    "print(\"Converted %d words (%d misses)\" % (hits, misses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9e985f01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Sample:\n",
      "  database ece jawaharlal microsoft 2014 kunam team 2014 bachelor technology indeed.com/r/ashok-kunam/7aac8767aacf10a0 2014 ashok kunam kunam indeed.com/r/ashok-kunam/7aac8767aacf10a0 2014 2014 ashok ece ashok kunam microsoft technology kunam 2014 microsoft indeed.com/r/ashok-kunam/7aac8767aacf10a0 lead 2014 database indeed.com/r/ashok-kunam/7aac8767aacf10a0 team nehru bachelor lead ashok jawaharlal team ashok database 2014 jawaharlal university kunam indeed.com/r/ashok-kunam/7aac8767aacf10a0 lead team lead 2014 microsoft 2014 microsoft database lead ashok 2014 2014 technology bachelor ashok 2014 ashok bachelor indeed.com/r/ashok-kunam/7aac8767aacf10a0 lead nehru ashok indeed.com/r/ashok-kunam/7aac8767aacf10a0 nehru indeed.com/r/ashok-kunam/7aac8767aacf10a0 indeed.com/r/ashok-kunam/7aac8767aacf10a0 technology indeed.com/r/ashok-kunam/7aac8767aacf10a0 university database university indeed.com/r/ashok-kunam/7aac8767aacf10a0 database 2014 technology 2014 university lead technology team bachelor nehru database kunam microsoft jawaharlal bachelor bachelor ashok university university indeed.com/r/ashok-kunam/7aac8767aacf10a0 university 2014 lead ashok bachelor team bachelor bachelor 2014 2014 university university ashok kunam ashok database kunam 2014 bachelor bachelor bachelor bachelor database bachelor database ashok bachelor bachelor database university bachelor database 2014 team 2014 bachelor nehru database team kunam team\n",
      "\n",
      " \n",
      "Raw Label:\n",
      "  B-Skills I-Degree B-College Name B-Companies worked at B-Graduation Year I-Name B-Designation B-Graduation Year B-Degree I-Degree B-Email Address B-Graduation Year B-Name I-Name I-Name B-Email Address B-Graduation Year B-Graduation Year B-Name I-Degree B-Name I-Name B-Companies worked at I-Degree I-Name B-Graduation Year B-Companies worked at B-Email Address I-Designation B-Graduation Year B-Skills B-Email Address B-Designation I-College Name B-Degree I-Designation B-Name B-College Name B-Designation B-Name B-Skills B-Graduation Year B-College Name I-College Name I-Name B-Email Address I-Designation B-Designation I-Designation B-Graduation Year B-Companies worked at B-Graduation Year B-Companies worked at B-Skills I-Designation B-Name B-Graduation Year B-Graduation Year I-Degree B-Degree B-Name B-Graduation Year B-Name B-Degree B-Email Address I-Designation I-College Name B-Name B-Email Address I-College Name B-Email Address B-Email Address I-Degree B-Email Address I-College Name B-Skills I-College Name B-Email Address B-Skills B-Graduation Year I-Degree B-Graduation Year I-College Name I-Designation I-Degree B-Designation B-Degree I-College Name B-Skills I-Name B-Companies worked at B-College Name B-Degree B-Degree B-Name I-College Name I-College Name B-Email Address I-College Name B-Graduation Year I-Designation B-Name B-Degree B-Designation B-Degree B-Degree B-Graduation Year B-Graduation Year I-College Name I-College Name B-Name I-Name B-Name B-Skills I-Name B-Graduation Year B-Degree B-Degree B-Degree B-Degree B-Skills B-Degree B-Skills B-Name B-Degree B-Degree B-Skills I-College Name B-Degree B-Skills B-Graduation Year B-Designation B-Graduation Year B-Degree I-College Name B-Skills B-Designation I-Name B-Designation\n",
      "\n",
      " \n",
      "After processing, sample:\n",
      " [1930 1921  858 ...    0    0    0]\n",
      "\n",
      " \n",
      "After processing, labels:\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "#Tagging word to index and tag to index\n",
    "\n",
    "#from keras.preprocessing.sequence import pad_sequences\n",
    "#from keras_preprocessing.sequence import pad_sequences\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Convert each sentence from list of Token to list of word_index\n",
    "X = [[word2idx[w[0]] for w in s] for s in sentence_all]\n",
    "\n",
    "# Padding each sentence to have the same lenght\n",
    "X = pad_sequences(maxlen=MAX_LEN, sequences=X, padding=\"post\", value=word2idx[\"PAD\"])\n",
    "\n",
    "# Convert Tag/Label to tag_index\n",
    "y = [[tag2idx[w[1]] for w in s] for s in sentence_all]\n",
    "\n",
    "# Padding each sentence to have the same lenght\n",
    "y = pad_sequences(maxlen=MAX_LEN, sequences=y, padding=\"post\", value=tag2idx[\"PAD\"])\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# One-Hot encode\n",
    "y = [to_categorical(i, num_classes=len(tags)+1) for i in y]  # n_tags+1(PAD)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.1)\n",
    "X_tr.shape, X_te.shape, np.array(y_tr).shape, np.array(y_te).shape\n",
    "\n",
    "\n",
    "print('Raw Sample:\\n ', ' '.join([w[0] for w in sentence_all[1]]))\n",
    "print('\\n ' )\n",
    "print('Raw Label:\\n ', ' '.join([w[1] for w in sentence_all[1]]))\n",
    "print('\\n ' )\n",
    "print('After processing, sample:\\n', X[0])\n",
    "print('\\n ' )\n",
    "print('After processing, labels:\\n', y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e9428a61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4441"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "44b31b60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((196, 3330), (22, 3330), (196, 3330, 22), (22, 3330, 22))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tr.shape, X_te.shape, np.array(y_tr).shape, np.array(y_te).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1e36b6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_tr = np.array(y_tr)\n",
    "y_te = np.array(y_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80f55f24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\punna\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow_addons\\utils\\tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'MAX_LEN' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 10\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_addons\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlosses\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SigmoidFocalCrossEntropy\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_addons\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptimizers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AdamW\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbuild_model\u001b[39m(max_len \u001b[38;5;241m=\u001b[39m \u001b[43mMAX_LEN\u001b[49m, input_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(word2idx),embedding_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m300\u001b[39m):\n\u001b[0;32m     11\u001b[0m   \u001b[38;5;66;03m# Model definition\u001b[39;00m\n\u001b[0;32m     12\u001b[0m   \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m Input(shape\u001b[38;5;241m=\u001b[39m(max_len,))\n\u001b[0;32m     14\u001b[0m   \u001b[38;5;66;03m# Get embeddings\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'MAX_LEN' is not defined"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional\n",
    "from tensorflow_addons.utils.types import FloatTensorLike, TensorLike\n",
    "from tensorflow_addons.layers import CRF\n",
    "from tensorflow_addons.losses import SigmoidFocalCrossEntropy\n",
    "from tensorflow_addons.optimizers import AdamW\n",
    "\n",
    "\n",
    "\n",
    "def build_model(max_len = MAX_LEN, input_dim = len(word2idx),embedding_dim = 300):\n",
    "  # Model definition\n",
    "  input = Input(shape=(max_len,))\n",
    "\n",
    "  # Get embeddings\n",
    "  embeddings = Embedding(\n",
    "      input_dim,\n",
    "      embedding_dim,\n",
    "      weights=[embedding_matrix],\n",
    "    input_length=max_len,\n",
    "       mask_zero=True,\n",
    "    trainable=True,\n",
    "      name = 'embedding_layer'\n",
    "  )(input)\n",
    "\n",
    "  # variational biLSTM\n",
    "  output_sequences = Bidirectional(LSTM(units=50, return_sequences=True))(embeddings)\n",
    "  output_sequences = Bidirectional(LSTM(units=100, return_sequences=True))(output_sequences)\n",
    "  # Stacking\n",
    "  output_sequences = Bidirectional(LSTM(units=50, return_sequences=True))(output_sequences)\n",
    "\n",
    "  # Adding more non-linearity\n",
    "  dense_out = TimeDistributed(Dense(25, activation=\"relu\"))(output_sequences)\n",
    "\n",
    "  # CRF layer\n",
    "  mask = Input(shape=(max_len,), dtype=tf.bool)\n",
    "  crf = CRF(22, name='crf')\n",
    "  predicted_sequence, potentials, sequence_length, crf_kernel = crf(dense_out)\n",
    "\n",
    "  model = Model(input, potentials)\n",
    "  model.compile(\n",
    "      optimizer=AdamW(weight_decay=0.001),\n",
    "      loss= SigmoidFocalCrossEntropy()) # Sigmoid focal cross entropy loss\n",
    "\n",
    "  return model\n",
    "\n",
    "model = build_model()\n",
    "\n",
    "# Checkpointing\n",
    "save_model = tf.keras.callbacks.ModelCheckpoint(filepath='ner_crf.h5',\n",
    "  monitor='val_loss',\n",
    "  save_weights_only=True,\n",
    "  save_best_only=True,\n",
    "  verbose=1\n",
    ")\n",
    "\n",
    "# Early stopping\n",
    "es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', verbose=1, patience=10)\n",
    "\n",
    "callbacks = [save_model, es]\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15057ee7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mfit(\n\u001b[0;32m      2\u001b[0m   X_tr,\n\u001b[0;32m      3\u001b[0m   y_tr,\n\u001b[0;32m      4\u001b[0m   batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m,\n\u001b[0;32m      5\u001b[0m   epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m,\n\u001b[0;32m      6\u001b[0m   validation_data\u001b[38;5;241m=\u001b[39m(X_te, y_te),\n\u001b[0;32m      7\u001b[0m   callbacks\u001b[38;5;241m=\u001b[39mcallbacks,\n\u001b[0;32m      8\u001b[0m   shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m      9\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "  X_tr,\n",
    "  y_tr,\n",
    "  batch_size=64,\n",
    "  epochs=20,\n",
    "  validation_data=(X_te, y_te),\n",
    "  callbacks=callbacks,\n",
    "  shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6da0e2f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
